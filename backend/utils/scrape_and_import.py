import os
import subprocess
import sys
import logging

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Paths
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
SCRAPY_DIR = os.path.join(BASE_DIR, 'webscraping')
OUTPUT_FILE = os.path.join(SCRAPY_DIR, 'new_precautions.json')

# Import the importer function directly
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from backend.utils.import_precautions import import_precautions

def scrape_and_import_disease(disease_name):
    """
    Runs the Scrapy spider for a specific disease and imports the data.
    """
    logger.info(f"Starting on-demand scrape for: {disease_name}")
    
    # 1. Run Scrapy (Blocking subprocess)
    # Command: scrapy crawl precautions -a disease="flu" -O new_precautions.json
    try:
        # We need to run inside the scrapy project directory
        cmd = [
            "scrapy", "crawl", "precautions",
            "-a", f"disease={disease_name}",
            "-O", "new_precautions.json" # Overwrite temp file
        ]
        
        # Check if we are in Docker or Local, adjustments might be needed for path
        # Assuming run from root or backend, we switch cwd to webscraping
        
        logger.info(f"Executing: {' '.join(cmd)}")
        result = subprocess.run(
            cmd,
            cwd=SCRAPY_DIR,
            capture_output=True,
            text=True
        )
        
        if result.returncode != 0:
            logger.error(f"Scrapy failed: {result.stderr}")
            return False
        
        logger.info("Scrapy finished successfully.")
        
        # 2. Import Data
        json_path = os.path.join(SCRAPY_DIR, "new_precautions.json")
        if os.path.exists(json_path):
            import_precautions(json_path)
            logger.info("Import completed.")
            
            # Clean up
            os.remove(json_path)
            return True
        else:
            logger.error("No JSON output generated by Scrapy.")
            return False

    except Exception as e:
        logger.error(f"Error in scrape_and_import: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) > 1:
        scrape_and_import_disease(sys.argv[1])
    else:
        print("Usage: python scrape_and_import.py <disease_name>")
